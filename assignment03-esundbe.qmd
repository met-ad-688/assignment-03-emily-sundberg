---
title: Assignment 03
author:
  - name: Emily Sundberg
    affiliations:
      - id: bu
        name: Boston University
        city: Boston
        state: MA
number-sections: true
date: '2025-09-24'
format:
  html:
    theme: cerulean
    toc: true
    toc-depth: 2
date-modified: today
date-format: long
---

# Load the Data Set

```{python}
import pandas as pd
import plotly.express as px
import plotly.io as pio
from pyspark.sql import SparkSession
import re
import numpy as np
import plotly.graph_objects as go
from pyspark.sql.functions import col, split, explode, regexp_replace, transform, when
from pyspark.sql import functions as F
from pyspark.sql.functions import col, monotonically_increasing_id

np.random.seed(42)

pio.renderers.default = "notebook"

spark = SparkSession.builder.appName("LightcastData").getOrCreate()

df = spark.read.option("header", "true").option("inferSchema", "true").option("multiLine","true").option("escape", "\"").csv("/home/ubuntu/assignment-03-emily-sundberg-1/data/lightcast_job_postings.csv")
df.createOrReplaceTempView("job_postings")

#print("---This is Diagnostic check, No need to print it in the final doc---")

#df.printSchema() # comment this line when rendering the submission
df.show(5)

```

# Data Cleaning

## Casting salary and experience columns
```{python}

df = df.withColumn("SALARY_FROM", col ("SALARY_FROM").cast("float"))\
  .withColumn("SALARY_TO", col("SALARY_TO").cast("float")) \
  .withColumn("MAX_YEARS_EXPERIENCE", col("MAX_YEARS_EXPERIENCE").cast("float"))\
  .withColumn("MIN_YEARS_EXPERIENCE", col("MIN_YEARS_EXPERIENCE").cast("float"))\
  .withColumn("SALARY", col("SALARY").cast("float"))
  

```

## Computing medians for salary columns
```{python}
def compute_median(sdf, col_name):
  q = sdf.approxQuantile(col_name, [0.5], 0.01)
  return q[0] if q else None


median_from = compute_median(df, "SALARY_FROM")
median_to = compute_median(df, "SALARY_TO")
median_salary = compute_median(df,"SALARY")

print("Medians:", median_from, median_to, median_salary)

```

## Impute missing salaries

```{python}
df = df.fillna({
  "SALARY_FROM": median_from,
  "SALARY_TO": median_to,
  "SALARY": median_salary
})
```

## Cleaning Education Column

```{python}
df = df.withColumn("EDUCATION_LEVELS_NAME", regexp_replace(col("EDUCATION_LEVELS_NAME"), "[\n\r]", ""))
ed = df.select("EDUCATION_LEVELS_NAME")
ed.show(15)

```

## Compute Average Salary

```{python}
df = df.withColumn("AVG_SALARY", (col("SALARY_FROM")+col("SALARY_TO"))/2)

```


## Exporting Cleaned Data

```{python}
df_selected = df.select(
  "EDUCATION_LEVELS_NAME",
  "REMOTE_TYPE_NAME",
  "MAX_YEARS_EXPERIENCE",
  "AVG_SALARY",
  "LOT_V6_SPECIALIZED_OCCUPATION_NAME",
  "NAICS2_NAME")

```

## Save to CSV

```{python}
pdf = df_selected.toPandas()

pdf.to_csv("./data/lightcast_cleaned.csv", index=False)

print("Data Cleaning Complete. Rows retained:", len(pdf))
```

# Salary Distribution by Industry and Employment Type

## Remove records where salary is missing or zero

```{python}
pdf = df.filter(df["SALARY"] >0).select("EMPLOYMENT_TYPE_NAME", "SALARY","NAICS2_NAME").toPandas()
pdf["EMPLOYMENT_TYPE_NAME"] = pdf["EMPLOYMENT_TYPE_NAME"].fillna("Unknown")
pdf["EMPLOYMENT_TYPE_NAME"] = pdf["EMPLOYMENT_TYPE_NAME"].apply(lambda x: re.sub(r"[^\x00-\x7F]+","",x))

pdf.head()

```

## Aggregate Data

```{python}

median_salaries = pdf.groupby("EMPLOYMENT_TYPE_NAME")["SALARY"].median()

sorted_employment_types = median_salaries.sort_values(ascending = False).index

pdf["EMPLOYMENT_TYPE_NAME"] = pd.Categorical(
  pdf["EMPLOYMENT_TYPE_NAME"],
  categories=sorted_employment_types,
  ordered=True
)

```

## Visualize Results

```{python}

import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px

fig = px.box(
  pdf,
  x = "NAICS2_NAME",
  y = "SALARY",
  #color = "EMPLOYMENT_TYPE_NAME",
  title = "Salary Distribution by Employment Type",
  color_discrete_sequence = ["peru"],
  boxmode = "group",
  points = "outliers" 
)



```

## Format Visualization

```{python}
fig.update_layout(
  title=dict(
    width=2000, 
    height=600,
    text = "Salary Distribution By Employment Type",
    font=dict(size=30, family="Montserrat", color = "saddlebrown", weight="bold")
  ),

  xaxis=dict(
    title=dict(text="NAICS2 Name", font=dict(size=14, family="Montserrat", color="saddlebrown", weight="bold")),
    tickangle=50,
    showline=True,
    linewidth=2,
    linecolor="saddlebrown",
    mirror=True,
    showgrid=False,
    categoryorder="array",
    categoryarray=sorted_employment_types.tolist()
  ),

  yaxis=dict(
    title=dict(text="Salary (K $)", font=dict(size=14, family="Montserrat", color="saddlebrown", weight="bold")),
    tickvals=[0,50000,100000,150000,200000,250000,300000,350000,400000,450000,500000],
    ticktext=["0","50K","100K","150K","200K","250K","300K","350K","400K","450K","500K"],
    tickfont=dict(size=12, family="Montserrat", color="saddlebrown",weight="bold"),
    showline=True,
    linewidth=2,
    linecolor="saddlebrown",
    mirror=True,
    showgrid=True,
    gridcolor="tan",
    gridwidth=0.5
  )
)

fig.show()
```


The plot above shows us that the medians of these industries average around 100K but there are several career fields with the opportunity to make significantly more. Both admin support and information have outliers in the $500K range. 

# Salary Analysis by ONET Occupation Type

## Aggregate Data

```{python}
salary_analysis = spark.sql("""
  SELECT
    TITLE_NAME AS ONET_NAME,
    PERCENTILE(SALARY, 0.5) AS Median_Salary,
    Count(*) AS Job_Postings
  FROM job_postings
  GROUP BY TITLE_NAME
  ORDER BY Job_Postings DESC
  LIMIT 10
""")

salary_pd = salary_analysis.toPandas()

```

## Bubble Plot

```{python}
fig = px.scatter(
  salary_pd,
  x="ONET_NAME",
  y="Median_Salary",
  size="Job_Postings",
  title ="Salary Analysis by ONET Occupation Type (Bubble Chart)"
)

fig.show()
```

This tells us that data analysts have a significant amount of job postings in the less than $100K range. 


